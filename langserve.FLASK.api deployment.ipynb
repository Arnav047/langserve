{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1e90a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langserve\n",
    "# !pip install fastapi\n",
    "# !pip install uvicorn\n",
    "# !pip install sse_starlette\n",
    "# !pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "816d6ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langserve import add_routes\n",
    "import uvicorn\n",
    "import os\n",
    "from langchain_community.llms import Ollama\n",
    "from dotenv import load_dotenv\n",
    "import nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7e1b1f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8143f11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGCHAIN_API_KEY=\"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\"\n",
    "OPEN_API_KEY=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29a35847",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY']=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94cc31cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "app=FastAPI(\n",
    "    title=\"Langchain Server\",\n",
    "    version=\"1.0\",\n",
    "    decsription=\"A simple API Server\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14ffaeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_9428\\573589718.py:3: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  ChatOpenAI(),\n"
     ]
    }
   ],
   "source": [
    "add_routes(\n",
    "    app,\n",
    "    ChatOpenAI(),\n",
    "    path=\"/openai\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a408f1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_9428\\1387318612.py:4: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm=Ollama(model=\"llama3.2\")\n"
     ]
    }
   ],
   "source": [
    "# openai llm model prompt 1\n",
    "model=ChatOpenAI()\n",
    "##ollama llama3.2 prompt 2\n",
    "llm=Ollama(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf8a2f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1=ChatPromptTemplate.from_template(\"Write me an essay about {topic} with 100 words\")\n",
    "prompt2=ChatPromptTemplate.from_template(\"Write me an poem about {topic} for a 5 years child with 100 words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "712f3db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_routes(\n",
    "    app,\n",
    "    prompt1|model,\n",
    "    path=\"/essay\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b38849ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_routes(\n",
    "    app,\n",
    "    prompt2|llm,\n",
    "    path=\"/poem\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94cda9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [9428]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://localhost:8001 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     __          ___      .__   __.   _______      _______. _______ .______     ____    ____  _______\n",
      "    |  |        /   \\     |  \\ |  |  /  _____|    /       ||   ____||   _  \\    \\   \\  /   / |   ____|\n",
      "    |  |       /  ^  \\    |   \\|  | |  |  __     |   (----`|  |__   |  |_)  |    \\   \\/   /  |  |__\n",
      "    |  |      /  /_\\  \\   |  . `  | |  | |_ |     \\   \\    |   __|  |      /      \\      /   |   __|\n",
      "    |  `----./  _____  \\  |  |\\   | |  |__| | .----)   |   |  |____ |  |\\  \\----.  \\    /    |  |____\n",
      "    |_______/__/     \\__\\ |__| \\__|  \\______| |_______/    |_______|| _| `._____|   \\__/     |_______|\n",
      "    \n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m Playground for chain \"/poem/\" is live at:\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  │\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  └──> /poem/playground/\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m Playground for chain \"/essay/\" is live at:\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  │\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  └──> /essay/playground/\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m Playground for chain \"/openai/\" is live at:\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  │\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  └──> /openai/playground/\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m See all available routes at /docs/\n",
      "INFO:     ::1:57440 - \"GET / HTTP/1.1\" 404 Not Found\n",
      "INFO:     ::1:57441 - \"GET /poem/playground/ HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:57441 - \"GET /poem/playground/assets/index-400979f0.js HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:57443 - \"GET /poem/playground/assets/index-52e8ab2f.css HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:57443 - \"POST /poem/stream_log HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:57450 - \"GET /essay/playground/ HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:57450 - \"GET /essay/playground/assets/index-400979f0.js HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:57451 - \"GET /essay/playground/assets/index-52e8ab2f.css HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:57452 - \"POST /essay/stream_log HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\sse_starlette\\sse.py\", line 244, in __call__\n",
      "    await cancel_on_finish(lambda: self._listen_for_disconnect(receive))\n",
      "  File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\sse_starlette\\sse.py\", line 233, in cancel_on_finish\n",
      "    await coro()\n",
      "  File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\sse_starlette\\sse.py\", line 175, in _listen_for_disconnect\n",
      "    message = await receive()\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 531, in receive\n",
      "    await self.message_event.wait()\n",
      "  File \"C:\\Python3.11\\Lib\\asyncio\\locks.py\", line 213, in wait\n",
      "    await fut\n",
      "  File \"C:\\Python3.11\\Lib\\asyncio\\futures.py\", line 287, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"C:\\Python3.11\\Lib\\asyncio\\tasks.py\", line 339, in __wakeup\n",
      "    future.result()\n",
      "  File \"C:\\Python3.11\\Lib\\asyncio\\futures.py\", line 198, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError: Cancelled by cancel scope 1cccbad6d10\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "  + Exception Group Traceback (most recent call last):\n",
      "  |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 403, in run_asgi\n",
      "  |     result = await app(  # type: ignore[func-returns-value]\n",
      "  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n",
      "  |     return await self.app(scope, receive, send)\n",
      "  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\fastapi\\applications.py\", line 1054, in __call__\n",
      "  |     await super().__call__(scope, receive, send)\n",
      "  |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\starlette\\applications.py\", line 112, in __call__\n",
      "  |     await self.middleware_stack(scope, receive, send)\n",
      "  |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 187, in __call__\n",
      "  |     raise exc\n",
      "  |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 165, in __call__\n",
      "  |     await self.app(scope, receive, _send)\n",
      "  |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 62, in __call__\n",
      "  |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "  |     raise exc\n",
      "  |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "  |     await app(scope, receive, sender)\n",
      "  |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\starlette\\routing.py\", line 715, in __call__\n",
      "  |     await self.middleware_stack(scope, receive, send)\n",
      "  |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\starlette\\routing.py\", line 735, in app\n",
      "  |     await route.handle(scope, receive, send)\n",
      "  |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\starlette\\routing.py\", line 288, in handle\n",
      "  |     await self.app(scope, receive, send)\n",
      "  |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\starlette\\routing.py\", line 76, in app\n",
      "  |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "  |     raise exc\n",
      "  |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "  |     await app(scope, receive, sender)\n",
      "  |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\starlette\\routing.py\", line 74, in app\n",
      "  |     await response(scope, receive, send)\n",
      "  |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\sse_starlette\\sse.py\", line 230, in __call__\n",
      "  |     async with anyio.create_task_group() as task_group:\n",
      "  |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 767, in __aexit__\n",
      "  |     raise BaseExceptionGroup(\n",
      "  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "  +-+---------------- 1 ----------------\n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"C:\\Python3.11\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    |     result = coro.send(None)\n",
      "    |              ^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\sse_starlette\\sse.py\", line 233, in cancel_on_finish\n",
      "    |     await coro()\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\sse_starlette\\sse.py\", line 154, in _stream_response\n",
      "    |     async for data in self.body_iterator:\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\langserve\\api_handler.py\", line 1277, in _stream_log\n",
      "    |     async for chunk in self._runnable.astream_log(\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 1107, in astream_log\n",
      "    |     async for item in _astream_log_implementation(  # type: ignore\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\langchain_core\\tracers\\log_stream.py\", line 675, in _astream_log_implementation\n",
      "    |     await task\n",
      "    |   File \"C:\\Python3.11\\Lib\\asyncio\\futures.py\", line 290, in __await__\n",
      "    |     return self.result()  # May raise too.\n",
      "    |            ^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Python3.11\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    |     raise self._exception.with_traceback(self._exception_tb)\n",
      "    |   File \"C:\\Python3.11\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    |     result = coro.send(None)\n",
      "    |              ^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\langchain_core\\tracers\\log_stream.py\", line 629, in consume_astream\n",
      "    |     async for chunk in runnable.astream(input, config, **kwargs):\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3426, in astream\n",
      "    |     async for chunk in self.atransform(input_aiter(), config, **kwargs):\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3409, in atransform\n",
      "    |     async for chunk in self._atransform_stream_with_config(\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 2295, in _atransform_stream_with_config\n",
      "    |     chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]\n",
      "    |                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Python3.11\\Lib\\asyncio\\futures.py\", line 287, in __await__\n",
      "    |     yield self  # This tells Task to wait for completion.\n",
      "    |     ^^^^^^^^^^\n",
      "    |   File \"C:\\Python3.11\\Lib\\asyncio\\tasks.py\", line 339, in __wakeup\n",
      "    |     future.result()\n",
      "    |   File \"C:\\Python3.11\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    |     raise self._exception.with_traceback(self._exception_tb)\n",
      "    |   File \"C:\\Python3.11\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    |     result = coro.send(None)\n",
      "    |              ^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\langchain_core\\tracers\\log_stream.py\", line 254, in tap_output_aiter\n",
      "    |     async for chunk in output:\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3379, in _atransform\n",
      "    |     async for output in final_pipeline:\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 1465, in atransform\n",
      "    |     async for output in self.astream(final, config, **kwargs):\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 509, in astream\n",
      "    |     async for chunk in self._astream(\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\langchain_community\\chat_models\\openai.py\", line 525, in _astream\n",
      "    |     async for chunk in await acompletion_with_retry(\n",
      "    |                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\langchain_community\\chat_models\\openai.py\", line 109, in acompletion_with_retry\n",
      "    |     return await llm.async_client.create(**kwargs)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1720, in create\n",
      "    |     return await self._post(\n",
      "    |            ^^^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    |     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    |     return await self._request(\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1623, in _request\n",
      "    |     return await self._retry_request(\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1670, in _retry_request\n",
      "    |     return await self._request(\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1623, in _request\n",
      "    |     return await self._retry_request(\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1670, in _retry_request\n",
      "    |     return await self._request(\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    |     raise self._make_status_error_from_response(err.response) from None\n",
      "    | openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "    +------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     ::1:57456 - \"POST /essay/stream_log HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\sse_starlette\\sse.py\", line 244, in __call__\n",
      "    await cancel_on_finish(lambda: self._listen_for_disconnect(receive))\n",
      "  File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\sse_starlette\\sse.py\", line 233, in cancel_on_finish\n",
      "    await coro()\n",
      "  File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\sse_starlette\\sse.py\", line 175, in _listen_for_disconnect\n",
      "    message = await receive()\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 531, in receive\n",
      "    await self.message_event.wait()\n",
      "  File \"C:\\Python3.11\\Lib\\asyncio\\locks.py\", line 213, in wait\n",
      "    await fut\n",
      "  File \"C:\\Python3.11\\Lib\\asyncio\\futures.py\", line 287, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"C:\\Python3.11\\Lib\\asyncio\\tasks.py\", line 339, in __wakeup\n",
      "    future.result()\n",
      "  File \"C:\\Python3.11\\Lib\\asyncio\\futures.py\", line 198, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError: Cancelled by cancel scope 1cccbe43b90\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "  + Exception Group Traceback (most recent call last):\n",
      "  |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 403, in run_asgi\n",
      "  |     result = await app(  # type: ignore[func-returns-value]\n",
      "  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n",
      "  |     return await self.app(scope, receive, send)\n",
      "  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\fastapi\\applications.py\", line 1054, in __call__\n",
      "  |     await super().__call__(scope, receive, send)\n",
      "  |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\starlette\\applications.py\", line 112, in __call__\n",
      "  |     await self.middleware_stack(scope, receive, send)\n",
      "  |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 187, in __call__\n",
      "  |     raise exc\n",
      "  |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 165, in __call__\n",
      "  |     await self.app(scope, receive, _send)\n",
      "  |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 62, in __call__\n",
      "  |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "  |     raise exc\n",
      "  |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "  |     await app(scope, receive, sender)\n",
      "  |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\starlette\\routing.py\", line 715, in __call__\n",
      "  |     await self.middleware_stack(scope, receive, send)\n",
      "  |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\starlette\\routing.py\", line 735, in app\n",
      "  |     await route.handle(scope, receive, send)\n",
      "  |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\starlette\\routing.py\", line 288, in handle\n",
      "  |     await self.app(scope, receive, send)\n",
      "  |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\starlette\\routing.py\", line 76, in app\n",
      "  |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "  |     raise exc\n",
      "  |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "  |     await app(scope, receive, sender)\n",
      "  |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\starlette\\routing.py\", line 74, in app\n",
      "  |     await response(scope, receive, send)\n",
      "  |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\sse_starlette\\sse.py\", line 230, in __call__\n",
      "  |     async with anyio.create_task_group() as task_group:\n",
      "  |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 767, in __aexit__\n",
      "  |     raise BaseExceptionGroup(\n",
      "  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "  +-+---------------- 1 ----------------\n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"C:\\Python3.11\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    |     result = coro.send(None)\n",
      "    |              ^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\sse_starlette\\sse.py\", line 233, in cancel_on_finish\n",
      "    |     await coro()\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\sse_starlette\\sse.py\", line 154, in _stream_response\n",
      "    |     async for data in self.body_iterator:\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\langserve\\api_handler.py\", line 1277, in _stream_log\n",
      "    |     async for chunk in self._runnable.astream_log(\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 1107, in astream_log\n",
      "    |     async for item in _astream_log_implementation(  # type: ignore\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\langchain_core\\tracers\\log_stream.py\", line 675, in _astream_log_implementation\n",
      "    |     await task\n",
      "    |   File \"C:\\Python3.11\\Lib\\asyncio\\futures.py\", line 290, in __await__\n",
      "    |     return self.result()  # May raise too.\n",
      "    |            ^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Python3.11\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    |     raise self._exception.with_traceback(self._exception_tb)\n",
      "    |   File \"C:\\Python3.11\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    |     result = coro.send(None)\n",
      "    |              ^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\langchain_core\\tracers\\log_stream.py\", line 629, in consume_astream\n",
      "    |     async for chunk in runnable.astream(input, config, **kwargs):\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3426, in astream\n",
      "    |     async for chunk in self.atransform(input_aiter(), config, **kwargs):\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3409, in atransform\n",
      "    |     async for chunk in self._atransform_stream_with_config(\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 2295, in _atransform_stream_with_config\n",
      "    |     chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]\n",
      "    |                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Python3.11\\Lib\\asyncio\\futures.py\", line 287, in __await__\n",
      "    |     yield self  # This tells Task to wait for completion.\n",
      "    |     ^^^^^^^^^^\n",
      "    |   File \"C:\\Python3.11\\Lib\\asyncio\\tasks.py\", line 339, in __wakeup\n",
      "    |     future.result()\n",
      "    |   File \"C:\\Python3.11\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    |     raise self._exception.with_traceback(self._exception_tb)\n",
      "    |   File \"C:\\Python3.11\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    |     result = coro.send(None)\n",
      "    |              ^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\langchain_core\\tracers\\log_stream.py\", line 254, in tap_output_aiter\n",
      "    |     async for chunk in output:\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3379, in _atransform\n",
      "    |     async for output in final_pipeline:\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 1465, in atransform\n",
      "    |     async for output in self.astream(final, config, **kwargs):\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 509, in astream\n",
      "    |     async for chunk in self._astream(\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\langchain_community\\chat_models\\openai.py\", line 525, in _astream\n",
      "    |     async for chunk in await acompletion_with_retry(\n",
      "    |                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\langchain_community\\chat_models\\openai.py\", line 109, in acompletion_with_retry\n",
      "    |     return await llm.async_client.create(**kwargs)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1720, in create\n",
      "    |     return await self._post(\n",
      "    |            ^^^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    |     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    |     return await self._request(\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1623, in _request\n",
      "    |     return await self._retry_request(\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1670, in _retry_request\n",
      "    |     return await self._request(\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1623, in _request\n",
      "    |     return await self._retry_request(\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1670, in _retry_request\n",
      "    |     return await self._request(\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\ADMIN\\myenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    |     raise self._make_status_error_from_response(err.response) from None\n",
      "    | openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "    +------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     ::1:57462 - \"GET /poem/playground/ HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:57462 - \"GET /poem/playground/assets/index-52e8ab2f.css HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:57463 - \"GET /poem/playground/assets/index-400979f0.js HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:57463 - \"POST /poem/stream_log HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [9428]\n"
     ]
    }
   ],
   "source": [
    "nest_asyncio.apply()\n",
    "if __name__==\"__main__\":\n",
    "    uvicorn.run(app,host=\"localhost\",port=8001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93534917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb157a25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c8b74e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python newENV",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
